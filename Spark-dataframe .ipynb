{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138035b0-c797-4be6-b896-abad32050d46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A PySpark DataFrame can be created via pyspark.sql.SparkSession.createDataFrame typically by passing a list of lists, tuples, dictionaries and pyspark.sql.Rows, a pandas DataFrame and an RDD consisting of such a list. pyspark.sql.SparkSession.createDataFrame takes the schema argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5228a064-c0f6-40ce-a9d5-e8f425694bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2e2233-2ade-44d9-9854-4b08baaddfe3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af3deaa-a061-48e5-851f-536d68421716",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\nonly showing top 1 row\n\n+-------+------------------+------------------+-------+\n|summary|                 a|                 b|      c|\n+-------+------------------+------------------+-------+\n|  count|                 3|                 3|      3|\n|   mean|2.3333333333333335|3.3333333333333335|   null|\n| stddev|1.5275252316519468|1.5275252316519468|   null|\n|    min|                 1|               2.0|string1|\n|    max|                 4|               5.0|string3|\n+-------+------------------+------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show(1)\n",
    "df.select(\"a\", \"b\", \"c\").describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2e9dd61-73af-4939-93c1-a65e44ce9838",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark-dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
